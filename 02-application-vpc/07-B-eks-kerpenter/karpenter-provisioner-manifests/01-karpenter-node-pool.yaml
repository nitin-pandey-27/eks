---
# https://karpenter.sh/docs/concepts/nodepools/
# https://github.com/aws/karpenter/tree/main/examples/v1beta1 
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: default
  namespace: ${namespace}
spec:
  # https://github.com/aws/karpenter/tree/main/examples/v1beta1
  # Template section that describes how to template out NodeClaim resources that Karpenter will provision
  # Karpenter will consider this template to be the minimum requirements needed to provision a Node using this NodePool
  # It will overlay this NodePool with Pods that need to schedule to further constrain the NodeClaims
  # Karpenter will provision to launch new Nodes for the cluster
  template:
    #metadata:
      # Labels are arbitrary key-values that are applied to all nodes
      #labels:
        #billing-team: my-team

      # Annotations are arbitrary key-values that are applied to all nodes
      #annotations:
        #example.com/owner: "my-team"
    spec:
      # References the Cloud Provider's NodeClass resource, see your cloud provider specific documentation
      nodeClassRef:
        group: karpenter.k8s.aws  # Updated since only a single version will be served
        kind: EC2NodeClass
        name: default

      # Provisioned nodes will have these taints
      # Taints may prevent pods from scheduling if they are not tolerated by the pod.

      #taints:
        #- key: example.com/special-taint
        #  effect: NoSchedule

      # Provisioned nodes will have these taints, but pods do not need to tolerate these taints to be provisioned by this
      # NodePool. These taints are expected to be temporary and some other entity (e.g. a DaemonSet) is responsible for
      # removing the taint after it has finished initializing the node.
      #startupTaints:
        #- key: example.com/another-taint
        #  effect: NoSchedule

      # Requirements that constrain the parameters of provisioned nodes.
      # These requirements are combined with pod.spec.topologySpreadConstraints, pod.spec.affinity.nodeAffinity, pod.spec.affinity.podAffinity, and pod.spec.nodeSelector rules.
      # Operators { In, NotIn, Exists, DoesNotExist, Gt, and Lt } are supported.
      # https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#operators
      # https://karpenter.sh/docs/reference/instance-types/#c52xlarge
      # # c5.2xlarge large - 8 core CPU / 16GB Memory - please refer above page for values
      requirements:
        - key: "karpenter.k8s.aws/instance-category"
          operator: In
          values: ["t"]
        - key: "karpenter.k8s.aws/instance-cpu"
          operator: In
          values: ["2"]  
        - key: "karpenter.k8s.aws/instance-family"
          operator: In
          values: ["t3"]
        - key: "karpenter.k8s.aws/instance-size"
          operator: In
          values: ["medium"]
        - key: "karpenter.k8s.aws/instance-memory"
          operator: In
          values: ["4096"]
        - key: "karpenter.k8s.aws/instance-hypervisor"
          operator: In
          values: ["nitro"]
        - key: "karpenter.k8s.aws/instance-generation"
          operator: Gt
          values: ["2"]
        - key: "topology.kubernetes.io/zone"
          operator: In
          values: ["${az_zone_1}", "${az_zone_2}"]
        - key: "kubernetes.io/arch"
          operator: In
          values: ["amd64"]
          #values: ["arm64", "amd64"]
        - key: "karpenter.sh/capacity-type"
          operator: In
          values: ["spot"]
          #values: ["spot", "on-demand"]
          ## https://karpenter.sh/v0.37/concepts/nodepools/ 
